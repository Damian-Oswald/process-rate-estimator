---
execute: 
  eval: false
---

# Sensitivity Analysis

Sensitivity analysis is a method used to determine how different values of an independent variable impact a particular dependent variable under a given set of assumptions.

By systematically varying key parameters, sensitivity analysis helps in identifying which variables have the most significant impact on the outcome, aiding in decision-making and risk assessment.

A sensitivity analysis is conducted in five steps:

1. Define parameter distribution.
2. Sample from parameter space.
3. Calculate model output of interest.
4. Approximate the model output based on inputs with an emulator.
5. Use the emulator to calculate parameter contributions to the output variance.

The first step is usually the hardest one, as it requires an extensive review of prior knowledge of the parameters.

Global sensitivity analysis examines the allocation of output uncertainty in a model (whether numerical or not) to various input uncertainty sources. The term "global" is specifically used to distinguish it from the more commonly encountered local or one-factor-at-a-time analyses found in the literature, as highlighted by @saltelli2004global.

## Parameter priors

:::{.column-page}
| End Member             |                                 | Value [‰] | Value reference         | Sampling                         | Range reference   |
|------------------------|---------------------------------|-----------|-------------------------|----------------------------------|-------------------|
| `eta_SP_diffusion`     | $\eta\: \text{SP}_\text{diff.}$ | 1.55      | @well2008isotope        | $\sim \mathcal{N}(1.55, 0.28^2)$ | @well2008isotope  |
| `eta_18O_diffusion`    | $\eta\ce{^{18}O}_\text{diff.}$  | -7.79     | @well2008isotope        | $\sim \mathcal{N}(-7.79, 0.27^2)$| @well2008isotope  |
| `SP_nitrification`     | $\text{SP}_\text{nit.}$         | 34.4      | @decock2013potential    | $\sim \mathcal{U}(26.2, 34.6)$   | @denk2017nitrogen |
| `d18O_nitrification`   | $\delta\ce{^{18}O_\text{nit.}}$ | 36.5      | @lewicka2017quantifying | $\sim \mathcal{N}(36.5, 2^2)$    |                   |
| `SP_denitrification`   | $\text{SP}_\text{den.}$         | -2.4      | @decock2013potential    | $\sim \mathcal{U}(-2.4, -0.9)$   | @denk2017nitrogen |
| `d18O_denitrification` | $\delta\ce{^{18}O_\text{den.}}$ | 11.1      | @lewicka2017quantifying | $\sim \mathcal{N}(11.1, 2^2)$    |                   |
| `eta_SP_reduction`     | $\eta\:\text{SP}_\text{red.}$   | -5.3      | @denk2017nitrogen       | $\sim \mathcal{U}(-8, -2)$       | @denk2017nitrogen |
| `eta_18O_reduction`    | $\eta\ce{^{18}O_\text{red.}}$   | -16.1     | @lewicka2017quantifying | $\sim \mathcal{U}(-24, -6)$      |                   |

: Isotope end members values used for modeling gross nitrification derived N~2~O production, gross denitrification derived N~2~O production, and gross N~2~O reduction. @lewicka2017quantifying originally report $\delta \ce{^{18} O \text - N2O} (\ce{N2O}/\ce{H2O})$. When a range is indicated, a random number from the range was selected in each iteration. When an average and standard deviation is indicated, a random value was drawn from the normal distribution in each iteration. {#tbl-sensitivity tbl-colwidths="[15,10,10,20,15,20]"}
:::

## Running the sensitivity analysis

The following section describes in detail how the sensitivity analysis is run. The script used to run the sensitivity analysis can be found in the respective GitHub repository. After successfully cloning the repository, the data for the sensitivity analysis can be reproduced simply by running

```bash
Rscript scripts/sensitivity-analysis/01-sensitivitiy-analysis.R
```

```{r}
COLUMNS = 1:12
DEPTHS = PRE::getParameters()$depths
SAMPLESIZE = 20
SAMPLEREPEAT = 3
n <- SAMPLESIZE * length(COLUMNS) * length(DEPTHS)
```

To conduct the sensitivity analysis, a data frame with parameters sampled according to the distributions in @tbl-sensitivity is created. We draw `n` parameter sets in total, where `n` is calculated based on the columns, depth layers and the `SAMPLESIZE` variable.

```{r}
parameters <- data.frame(
    expand.grid(repetition = 1:SAMPLESIZE,
                column = COLUMNS,
                depth = DEPTHS)[sample(1:n),-1],
    BD = sort(rep(getParameters()$BD+seq(-0.1,0.1,l=11),l=n)),
    eta_SP_diffusion = rnorm(n, 1.55, 0.28),
    eta_18O_diffusion = rnorm(n, -7.79, 0.27),
    SP_nitrification = runif(n, 32, 38.7),
    d18O_nitrification = rnorm(n, 23.5, 3),
    SP_denitrification = runif(n, -13.6, 3.7),
    d18O_denitrification = rnorm(n, 11.1, 2),
    eta_SP_reduction = runif(n,-8,-2),
    eta_18O_reduction = runif(n,-18,-12)
    )
```

Note that the experimental parameter grid (containing information about column and depth layer) is shuffled randomly. We do this to prevent columns or depths to correlate with the bulk density (`BD`). The bulk density parameter needs to be sorted, since it's not a parameter that's passed to the PRE model, but rather one that is used to prepare the data used in the model. In order to reduce the number of times needed to prepare said data, we'll sort `BD` and we'll only update the data if `BD` changes.

Next, we write a function `f` that will run the process rate estimator for a specified column and depth with the parameters in the vector `p` --- which coincides to one row in the `parameters` data frame.

```{r}
results <- data.frame(Nitrification = rep(NA, n),
                      Denitrification = rep(NA, n),
                      Reduction = rep(NA, n))
BD <- 0
for (i in 1:n) {
    
    if(!(parameters[i,"BD"]==BD)){ #<1>
        BD <- parameters[i,"BD"]
        P <- getParameters(BD = BD) #<2>
        data <- PRE::measurements |>
            getN2ON(P) |>
            getMissing() |>
            calculateFluxes(P, FALSE) #<2>
    }
    
    P <- do.call(getParameters, as.list(parameters[i,-(1:3)]))
    results[i,] <- longPRE(data = data,
                           column = parameters[i,"column"],
                           depth = parameters[i,"depth"],
                           n = SAMPLEREPEAT,
                           parameters = P,
                           verbose = FALSE)[["processes"]]
    
}
```
1. Check if the bulk density value `BD` has changed since the last iteration.
2. Prepare the `measurements` data with a new bulk density value `BD`.

That function makes it very straightforward to compute the data for the sensitivity analysis. We'll simply apply `f` to every single row in `parameters` using the `apply` function [@wickham2011split].

To conduct the sensitivity analysis, we now model the results based on the sampled parameters -- so we treat the sampled parameters as independent variables $\mathbf X$, and the computed process rates as dependent variables $\mathbf Y$.

A commonly applied method of sensitivity analysis is Sobol's method, which decomposes the total variance and assigns it to individual parameters specific interactions [@sobol1990sensitivity]. However, Sobol's method require's a large number of resampling, and is thus unfit for this application. Instead, a linear regression model is used as an emulator. Often, this works fine enough because even though the reponse might not be linear overall, it can be approximated by a linear model well enough on a specific parameter interval.

First, we might look at the pairwise correlations between columns of $\mathbf X$ and $\mathbf Y$, i.e. all combinations of process rates as well as isotope end members (@fig-sensitivity).

:::{.column-screen-inset}
![Pairwise results of the process rates (y-axis) and the isotope end members (x-axis) for column 1 at 7.5 cm depth. The red lines indicate the respective linear models with their 95% conficence intervals. The black bar to the right of each plot represents the magnitude of the absolute standardized regression coefficient (SRC). The absolute SRC can range from 0 to 1, where an SRC of 1 would range the entire plot plane. As such, these bars represent the importance of a given parameter for emulating the respective process.](scripts/sensitivity-analysis/output/sensitivity-1-7.5.svg){#fig-sensitivity width=100%}
:::

@fig-sensitivity already reveals quite a lot, but it would be better to construct a multiple linear regression model where we include all isotope end members at the same time.

The table below shows the resulting coefficients of such a multiple linear regression model. We can directly interpret the regression coefficients without any further modifications, as all considered variables share the same unit (‰).

### Looking at the raw coefficients

What do the coefficients $\boldsymbol \beta$ themselves mean? How can we interpret them?

```{r}
#| label: tbl-coefficients
#| results: asis
#| tbl-cap: Coefficients ($\mu$ ± $\sigma$) for the models fit on combinations of depth and column.
#| echo: false
#| eval: true
#| tbl-colwidths: [10,30,30,30]
results <- read.csv("scripts/sensitivity-analysis/output/coefficients.csv")
processes <- c("Nitrification","Denitrification","Reduction")
parameters <- c("BD", "eta_SP_diffusion", "eta_18O_diffusion", "SP_nitrification", "d18O_nitrification", "SP_denitrification", "d18O_denitrification", "eta_SP_reduction", "eta_18O_reduction")
results$Process <- ordered(results$Process, levels = processes)
results$Parameter <- ordered(results$Parameter, levels = parameters)
f <- function(x) paste(signif(mean(x, na.rm = TRUE),2),"±",signif(sd(x, na.rm = TRUE),2))
df <- with(results, tapply(Coefficient, list(Parameter, Process), f))
df <- rbind(df, strrep("",3), adj.R2 = with(results, tapply(adjR2, Process, f)))
df <- data.frame(c("Bulk density",
                   "$\\eta\\;\\text{SP}_{\\text{diffusion}}$",
                   "$\\eta^{18}\\text{O}_\\text{diffusion}$",
                   "$\\text{SP}_\\text{nitrification}$",
                   "$\\delta\\;^18\\text{O}_\\text{nitrification}$",
                   "$\\text{SP}_\\text{denitrification}$",
                   "$\\delta\\;^18\\text{O}_\\text{denitrification}$",
                   "$\\eta\\;\\text{SP}_{\\text{reduction}}$",
                   "$\\eta^{18}\\text{O}_\\text{reduction}$",
                   "",
                   "adjusted R^2^"), df)
colnames(df)[1] <- ""
knitr::kable(df, align = "lrrr", row.names = FALSE)
```

:::{.column-screen-inset-right}
![Results of the coefficients for all processes and parameters. The mangitude of the coefficients can be interpreted as the effect size said parameter would have without considering its uncertainty.](scripts/sensitivity-analysis/output/Coefficients.svg){#fig-coefficients width=100%}
:::

### Standardized regression coefficients

The standardized regression coefficient (SRC, $\beta^\ast$), often referred to as beta coefficient in statistical models, measures the strength and direction of the relationship between an independent variable and the dependent variable in a regression model, with all variables being standardized. The SRC can also be simply calculated as:

$$\beta^\ast_i = \frac{\sigma_{x_i}}{\sigma_y} \beta_i$${#eq-src}

where $\beta_i$ is the estimated coefficient of a predictor $x_i$ in a linear model, $\sigma_{x_i}$ is the predictor's standard deviation, while $\sigma_{y}$ is the response's standard deviation.
A consequence of standardizing the coefficients is that they all fall into the range between -1 and 1.

The key advantage of using standardized coefficients is that it allows for direct comparison of the relative importance or impact of each independent variable on the dependent variable across different studies or models, even when the variables are measured on different scales.
This facilitates a clearer understanding of which variables have the most significant influence on the outcome, making it a useful tool in many fields where interpreting the magnitude of predictive relationships is critical.

Standardized regression coefficients are especially useful for sensitivity analyses because they provide a scale-independent measure of the relative importance of each predictor variable in the model. Sensitivity analysis aims to understand how changes in input variables affect the output of a model.
They facilitate this by quantifying the effect size of each variable in a comparable way.

Since the coefficients are standardized, they remove the unit dependence that can obscure the interpretation of the coefficients in models involving variables measured on different scales or units. This standardization allows researchers to directly compare the influence of each variable, making it clearer which variables are more sensitive or have a greater impact on the outcome variable (@tbl-src).

```{r}
#| label: tbl-src
#| results: asis
#| tbl-cap: Standardized regression coefficients ($\mu$ ± $\sigma$) for the models fit on combinations of depth and column.
#| echo: false
#| eval: true
#| tbl-colwidths: [10,30,30,30]
f <- function(x) paste(signif(mean(x, na.rm = TRUE),2),"±",signif(sd(x, na.rm = TRUE),2))
df <- with(results, tapply(SRC, list(Parameter, Process), f))
df <- rbind(df, strrep("",3), adj.R2 = with(results, tapply(adjR2, Process, f)))
df <- data.frame(c("Bulk density",
                   "$\\eta\\;\\text{SP}_{\\text{diffusion}}$",
                   "$\\eta^{18}\\text{O}_\\text{diffusion}$",
                   "$\\text{SP}_\\text{nitrification}$",
                   "$\\delta\\;^18\\text{O}_\\text{nitrification}$",
                   "$\\text{SP}_\\text{denitrification}$",
                   "$\\delta\\;^18\\text{O}_\\text{denitrification}$",
                   "$\\eta\\;\\text{SP}_{\\text{reduction}}$",
                   "$\\eta^{18}\\text{O}_\\text{reduction}$",
                   "",
                   "adjusted R^2^"), df)
colnames(df)[1] <- ""
knitr::kable(df, align = "lrrr", row.names = FALSE)
```

Standardizing the coefficients changes the results seen in [table @tbl-coefficients] and [figure @fig-coefficients].

:::{.column-screen-inset-right}
![Results of the standardized regression coefficients (SRC) for all processes and parameters. The mangitude of the SRC can be interpreted as the effect size of said parameter: by how much does its uncertainty contribute to the overall uncertainty?](scripts/sensitivity-analysis/output/SRC.svg){#fig-src width=100%}
:::

Note that the coefficients of column, depth and their interactions are excluded from this table, even though they are more explanatory than the isotope end members.

$$
\frac{1}{p} \sum_{i=1}^p \mid \beta^\ast_p \mid
$$

![Overall results of the sensitivity analysis. The three show the share of variance explained by each parameter, calculated from the absolute standardized regression coefficients. Note that linear models did not explain the variance equally well for all three processes. The hatched area represents the variance unexplained by the linear models, i.e. 1 - R^2^.](scripts/sensitivity-analysis/output/SRC-importances.svg){#fig-src width=100%}

The results shown in [figure @fig-src] allow us to assign percentage-wise contributions to the variance explained to individual parameters tested in the sensitivity analysis. Averaged over all processes, $\eta\ce{^{18}O_\text{reduction}}$ contributes most to the variance in the processes (30.0%), followed by the bulk density (21.6%) and $\text{SP}_\text{denitrification}$ (15.1%).

However, not every parameter explains the variance in the process rate estimates equally well for different processes. For example, the variance in the estimated reduction rates are reasonably well explained (64.6%) only by bulk density and $\eta\ce{^{18}O_\text{reduction}}$ alone. Bulk density seems much less capable in explaining the variance in the estimated nitrification and denitrification rates, on the other hand. For these two processes, $\text{SP}_\text{denitrification}$ and $\eta\;\text{SP}_\text{reduction}$ seem to be the bigger driving factors.

## Using a mixed effects model as an emulator

A mixed effects model is a statistical model containing both fixed effects and random effects. In matrix notation, a linear mixed effects model can be represented as follows.
$$
\mathbf y = \mathbf X \boldsymbol \beta + \mathbf Z \boldsymbol \zeta  + \boldsymbol \varepsilon
$${#eq-mem}
In this notation, $\mathbf y$ is a known vector of obervations, where $\mathbb E (\mathbf y) = \mathbf X \boldsymbol \beta$. Unlike in the OLS model, the errors of this mixed effects model are derived from two sources: on one hand from the error vector $\boldsymbol \varepsilon$, but on the other hand also from the random effects $\mathbf Z \boldsymbol \zeta$, where we have both $\mathbb E (\boldsymbol \varepsilon) = \mathbf 0$ and $\mathbb E (\boldsymbol \zeta) = \mathbf 0$. So, generally, the expected error in the model is still zero. However, by use of the term $\mathbf Z \boldsymbol \zeta$, errors might be clustered into groups, where $\mathbf Z$ is a design matrix containing information about these groups.

```{r}
#| echo: false
#| eval: true
#| results: asis
cat(readLines("resources/tbl-sensitivity.txt"))
```

*One interpretation of the fixed effects coefficients $\boldsymbol \beta$ is that they are the estimated population means of the coefficient distributions of the random effects coefficients* [@bates2014fitting].

For approximating the synthetic data within the scope of the sensitivity analysis, we estimate three mixed effects models -- one for Nitrification, Denitrification and Reduction, respectively -- with random intercepts and random slopes. Thus, $3 \cdot 12 \cdot 5 \cdot 2 \cdot 8 + 3 \cdot 2 \cdot 8 = 2928$ parameters are estimated in total.
